{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2023aiml537/capstone_har_model/blob/main/Human_Activity_Recognition_Using_Smartphones_Mid_term.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "BNVZN1LCMbum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0Z7YUsLLu5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing data.** Since we are using google colab and data is in google drive so we need to allow colab to read data."
      ],
      "metadata": {
        "id": "Yd3j26-RQzBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "fKUcy_I2RipD",
        "outputId": "25e7601b-aeef-4530-8ced-a3a3c5595149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading data into data frame."
      ],
      "metadata": {
        "id": "77hKarguR4Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/AI_ML_Project/dataset/train.csv')"
      ],
      "metadata": {
        "id": "lPfuMm3iRwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KxzUbgSZSGVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Current train set size.\n",
        "df.shape"
      ],
      "metadata": {
        "id": "-3USFsHyUiNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Current train data set dimensions : (7352, 563)"
      ],
      "metadata": {
        "id": "mB4gDScTUq5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X_train = df.drop('Activity', axis=1)\n",
        "y_train = df['Activity']"
      ],
      "metadata": {
        "id": "v20OJuxvTyrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "RyqqfHOfT_k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "jFeRBAotUG_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading test data set."
      ],
      "metadata": {
        "id": "S8fuNAg0Vzia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/AI_ML_Project/dataset/test.csv')"
      ],
      "metadata": {
        "id": "zdg7okbaV-Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "nGFzpYdPWLjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "id": "Ktoy3tYJXA92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target\n",
        "X_test = df_test.drop('Activity', axis=1)\n",
        "y_test = df_test['Activity']"
      ],
      "metadata": {
        "id": "m8Efy3ePWSah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "dg7jfZZ6WZ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.head()"
      ],
      "metadata": {
        "id": "EYnnsKFbWeIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering**"
      ],
      "metadata": {
        "id": "iFcelVukc14J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Duplicate Features** : Different features (or columns) in a dataset that have the same or very similar values. These redundant features can lead to inefficiencies in model training and can also cause overfitting, where the model learns noise in the data rather than the underlying patterns."
      ],
      "metadata": {
        "id": "dMXoq-HYSsb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_duplicate_columns(df):\n",
        "\n",
        "    duplicate_columns = {}\n",
        "    seen_columns = {}\n",
        "\n",
        "    for column in df.columns:\n",
        "        current_column = df[column]\n",
        "\n",
        "        # Convert column data to bytes\n",
        "        try:\n",
        "            current_column_hash = current_column.values.tobytes()\n",
        "        except AttributeError:\n",
        "            current_column_hash = current_column.to_string().encode()\n",
        "        if current_column_hash in seen_columns:\n",
        "            if seen_columns[current_column_hash] in duplicate_columns:\n",
        "                duplicate_columns[seen_columns[current_column_hash]].append(column)\n",
        "            else:\n",
        "                duplicate_columns[seen_columns[current_column_hash]] = [column]\n",
        "        else:\n",
        "            seen_columns[current_column_hash] = column\n",
        "\n",
        "    return duplicate_columns"
      ],
      "metadata": {
        "id": "ebAhOUX2TBBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_columns = get_duplicate_columns(X_train)\n",
        "duplicate_columns"
      ],
      "metadata": {
        "id": "VlT1nFJnTevA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Taking a look into the duplicate column values."
      ],
      "metadata": {
        "id": "f2GPQUpaUVo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[['tBodyAccMag-mean()','tBodyAccMag-sma()','tGravityAccMag-mean()','tGravityAccMag-sma()']]"
      ],
      "metadata": {
        "id": "3xDdd571U9qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[['tBodyAccMag-std()','tBodyAccMag-std()']]"
      ],
      "metadata": {
        "id": "Ya1Cw-m2VJmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can see that most of the data in various columns across the data set is duplicated, we can drop the redundant column."
      ],
      "metadata": {
        "id": "Z_0IDCMBVWje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for duplicate_columns_list in duplicate_columns.values():\n",
        "    X_train.drop(columns=duplicate_columns_list,inplace=True)\n",
        "    X_test.drop(columns=duplicate_columns_list,inplace=True)"
      ],
      "metadata": {
        "id": "G9rFOaVdVkvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "q8ja5CnLXJ5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After removing duplicate columns the data set reduced from 562 columns to 541 columns."
      ],
      "metadata": {
        "id": "a8OpePXKXXen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variance Threshold** : It is used to remove features with low variance, which means they do not vary much and thus do not provide useful information for explaining the variation in the output.This method is applied to two types of features:\n",
        " - Constant Features: These are features where all values are the same.\n",
        " - Quasi-Constant Features: These features have the same value for the vast majority of rows (e.g., 995 out of 1000 rows have the same value), with only a few different values."
      ],
      "metadata": {
        "id": "L9P-EH3VX7l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel = VarianceThreshold(threshold=0.05)\n",
        "sel.fit(X_train)"
      ],
      "metadata": {
        "id": "4-jxMg-5YTds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sel.get_support()"
      ],
      "metadata": {
        "id": "yxODaiQMZMnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = X_train.columns[sel.get_support()]\n",
        "columns"
      ],
      "metadata": {
        "id": "ZDe6KbROZb_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = sel.transform(X_train)\n",
        "X_test = sel.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=columns)\n",
        "X_test = pd.DataFrame(X_test, columns=columns)"
      ],
      "metadata": {
        "id": "MfqHQCYLZwEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "0CTUc7NIZ1Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "HmGfcxYIaDqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pearson Correlation** in Removing Multicollinearity"
      ],
      "metadata": {
        "id": "f0tBdgCvaqr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(X_train.corr())"
      ],
      "metadata": {
        "id": "dsWQ0x1ravVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = X_train.corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "Up6jSGNMbOTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_correlated_columns(corr_matrix):\n",
        "  # Get the column names of the DataFrame\n",
        "  columns = corr_matrix.columns\n",
        "\n",
        "  # Create an empty list to keep track of columns to drop\n",
        "  columns_to_drop = []\n",
        "\n",
        "  # Loop over the columns\n",
        "  for i in range(len(columns)):\n",
        "      for j in range(i + 1, len(columns)):\n",
        "          # Access the cell of the DataFrame\n",
        "          if corr_matrix.loc[columns[i], columns[j]] > 0.95:\n",
        "              columns_to_drop.append(columns[j])\n",
        "\n",
        "  return columns_to_drop"
      ],
      "metadata": {
        "id": "4Ovc6tTcbxll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = get_correlated_columns(corr_matrix)\n",
        "columns_to_drop = set(columns_to_drop)\n",
        "len(columns_to_drop)"
      ],
      "metadata": {
        "id": "O0god4licNvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.drop(columns = columns_to_drop, axis = 1, inplace=True)\n",
        "X_test.drop(columns = columns_to_drop, axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "jNEja0yWcrSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "JFqgNmOodCyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ANOVA Test**"
      ],
      "metadata": {
        "id": "gsEy184EddxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel = SelectKBest(f_classif, k=100).fit(X_train, y_train)\n",
        "\n",
        "# display selected feature names\n",
        "X_train.columns[sel.get_support()]"
      ],
      "metadata": {
        "id": "NiHDsMC4dcYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = X_train.columns[sel.get_support()]\n",
        "\n",
        "X_train = sel.transform(X_train)\n",
        "X_test = sel.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=columns)\n",
        "X_test = pd.DataFrame(X_test, columns=columns)"
      ],
      "metadata": {
        "id": "swRqC9dAd3gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "j5Zsm4Bwd7sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eventually after feature engineering the total feature is reduced from 563 to 100."
      ],
      "metadata": {
        "id": "ROgWvmq1eGEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Columns that are considered for model building"
      ],
      "metadata": {
        "id": "l1fSJZbYDpX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=0\n",
        "for column in X_train.columns:\n",
        "  print(index,' - ',column)\n",
        "  index+=1"
      ],
      "metadata": {
        "id": "ZcFM20OnDyrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=pd.DataFrame(data, columns=column)"
      ],
      "metadata": {
        "id": "nCLtLeZQ0lel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwg7ACWY2CyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building models** using above features."
      ],
      "metadata": {
        "id": "C05jnlfHe3mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "y_test = le.transform(y_test)"
      ],
      "metadata": {
        "id": "jjbRegM7fA-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "9PRZ54ahZsmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)  # Increase max_iter if it doesn't converge\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "UDGCyQWGgSMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "e3mJiZ9ZRd6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test,y_pred)\n",
        "pd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicted'], margins = True)"
      ],
      "metadata": {
        "id": "rDKkubPLR9I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "width = 6\n",
        "height = 6\n",
        "n_classes = 6\n",
        "# Output classes to learn how to classify\n",
        "LABELS = le.classes_\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    confusion_matrix(y_test,y_pred),\n",
        "    interpolation='nearest',\n",
        "    cmap=plt.cm.rainbow\n",
        ")\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r4t0OD2CVuRn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}